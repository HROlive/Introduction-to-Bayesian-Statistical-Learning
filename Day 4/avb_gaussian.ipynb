{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytic Variational Bayes. Inferring a single Gaussian\n",
    "=====================\n",
    "\n",
    "This notebook implements the example from section 3 of the FMRIB tutorial on Variational Bayes (\"Inferring a single Gaussian\").\n",
    "\n",
    "We assume we have data drawn from a Gaussian distribution with true mean $\\mu$ and true precision $\\beta$:\n",
    "\n",
    "$$\n",
    "P(y_n | \\mu, \\beta) = \\frac{\\sqrt{\\beta}}{\\sqrt{2\\pi}} \\exp{-\\frac{\\beta}{2} (y_n - \\mu)^2}\n",
    "$$\n",
    "\n",
    "One interpretation of this is that our data consists of repeated measurements of a fixed value ($\\mu$) combined with Gaussian noise with standard deviation $\\frac{1}{\\sqrt{\\beta}}$.\n",
    "\n",
    "Here's how we can generate some sample data from this model in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples are:\n",
      "[39.99892592 41.40593071 42.31103574 42.1961179  42.58650678 42.44098922\n",
      " 42.21573311 40.7101928  40.6996739  41.00870367 42.17647774 40.65313638\n",
      " 40.03433765 42.59629228 43.2695579  44.10454499 41.03869608 41.83348026\n",
      " 40.73225742 44.21028809 41.02886628 41.12812731 41.78533388 42.85862706\n",
      " 44.05114161 44.74170784 39.27980311 40.77260831 44.31843522 43.66765353\n",
      " 42.63864911 39.91806054 42.55086012 44.09719662 43.32678961 40.60447457\n",
      " 40.51722229 44.18178014 40.8729958  42.43917026 43.16483738 42.37656479\n",
      " 41.50917433 39.82088879 41.01060765 41.88775222 41.35391432 40.99954258\n",
      " 39.22136559 43.33610632 41.62260604 42.91127244 43.36280234 38.73465996\n",
      " 43.49763631 41.1273821  43.2739239  40.93853426 41.13894747 42.44284628\n",
      " 40.93919441 41.69440534 39.16482931 43.33697601 42.70465162 42.76899921\n",
      " 41.36682527 42.44923562 44.64235387 44.62937672 41.33284928 42.01246787\n",
      " 38.96722831 42.72947699 40.20780677 42.36983721 42.65743891 42.953528\n",
      " 41.54924006 40.26894316 40.04468715 41.56150075 43.46963979 43.2717799\n",
      " 42.2930702  41.87140345 40.03470361 41.78567354 42.99590299 43.33644355\n",
      " 43.38202141 40.43346253 43.72380991 45.58329297 40.82559991 40.23754948\n",
      " 41.20033153 43.55676604 42.22185785 43.06281685]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Ground truth parameters\n",
    "# We infer the precision, BETA, but it is useful to\n",
    "# derive the variance and standard deviation from it\n",
    "MU_TRUTH = 42\n",
    "BETA_TRUTH = 0.5\n",
    "VAR_TRUTH = 1/BETA_TRUTH\n",
    "STD_TRUTH = np.sqrt(VAR_TRUTH)\n",
    "\n",
    "# Observed data samples are generated by Numpy from the ground truth\n",
    "# Gaussian distribution. Reducing the number of samples should make\n",
    "# the inference less 'confident' - i.e. the output variances for\n",
    "# MU and BETA will increase\n",
    "N = 100\n",
    "DATA = np.random.normal(MU_TRUTH, STD_TRUTH, [N])\n",
    "print(\"Data samples are:\")\n",
    "print(DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'signal + noise' interpretation we can view this as noisy measurements (red crosses) of a constant signal (green line):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14990c99d580>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(DATA, \"rx\")\n",
    "plt.plot([MU_TRUTH] * N, \"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Variational Bayes we make the approximation that the posterior is factorised with respect to these two parameters:\n",
    "\n",
    "$$q(\\mu, \\beta) = q(\\mu)q(\\beta)$$\n",
    "\n",
    "Generally it is a requirement for the analytic formulation of Variational Bayes that the 'noise' and 'signal' parameters are factorised. In more complex examples where there is more than one 'signal' parameter (e.g. we are inferring the parameters of a complex nonlinear model), a combined distribution may be used for the multiple signal parameters, however the noise must still be factorised.\n",
    "\n",
    "Another requirement for analytic VB is the choice of 'conjugate' distributions for the priors $P(\\mu)$, $P(\\beta)$, and the posteriors $q(\\mu)$ and $q(\\beta)$. This arises from Bayes's theorem which in our approximate form is:\n",
    "\n",
    "$$q(\\mu)q(\\beta) \\propto P(\\textbf{Y} | \\mu, \\beta)P(\\mu)P(\\beta)$$\n",
    "\n",
    "Here, $P(\\textbf{Y} | \\mu, \\beta)$ is the likelihood and is determined from the Gaussian data model given above. It turns out that if we choose certain types of distribution for the priors $P(\\mu)$ and $P(\\beta)$, then $q(\\mu)$ and $q(\\beta)$ will end up having the same type of distribution. These 'special' distributions are known as the 'conjugate' distributions *for the likelihood*. Conjugate distributions depend on the exact form of the likelihood function.\n",
    "\n",
    "We will not prove the conjugate distributions for this likelihood, but will simply state that for a Gaussian data model as above, the conjugate distribution for $\\mu$ is Gaussian, the the conjugate distribution for $\\beta$ is a Gamma distribution:\n",
    "\n",
    "$$P(\\mu) = \\frac{1}{\\sqrt{2\\pi v_0}} \\exp{-\\frac{1}{2v_0}(\\mu - m_0)^2}$$\n",
    "$$q(\\mu) = \\frac{1}{\\sqrt{2\\pi v}} \\exp{-\\frac{1}{2v}(\\mu - m)^2}$$\n",
    "\n",
    "Here $m$ and $v$ are the 'hyperparameters' of the posterior for $\\mu$ - they determine the inferred posterior distribution of \n",
    "$\\mu$, and through the VB formulation we will infer values for them from the data. $m_0$ and $v_0$ similarly describe our prior knowledge of the likely value of $\\mu$ and might incorporate existing knowledge. Alternatively by choosing a large value of $v_0$ we can have a 'non-informative' prior which would be used if we have no real idea before seeing the data what the value of $\\mu$ might be.\n",
    "\n",
    "Similarly for $\\beta$ we have:\n",
    "\n",
    "$$P(\\beta) = \\frac{1}{\\Gamma(c_0)}\\frac{\\beta^{c_0-1}}{b_0^{c_0}}\\exp{-\\frac{\\beta}{b_0}}$$\n",
    "$$q(\\beta) = \\frac{1}{\\Gamma(c)}\\frac{\\beta^{c-1}}{b^c}\\exp{-\\frac{\\beta}{b}}$$\n",
    "\n",
    "Here $b$ and $c$ are the inferred hyperparameters and $b_0$ and $c_0$ are the prior scale and shape parameters for the Gamma distribution. The mean of the Gamma distribution is given by $cb$ and the variance by $cb^2$. \n",
    "\n",
    "Sometimes it may be more intuitive to think of the Gamma prior in terms of a mean and variance in which case we can derive the prior hyperparameters as:\n",
    "\n",
    "$$b_0 = \\frac{\\textrm{Prior variance}}{\\textrm{Prior mean}}$$\n",
    "$$c_0 = \\frac{\\textrm{Prior mean}^2}{\\textrm{Prior variance}}$$\n",
    "\n",
    "Here we'll define some non-informative priors for $\\mu$ and $\\beta$. Note that for the noise prior, $\\beta$ we define the prior mean and variance and derive the prior hyperparameters $b_0$ and $c_0$ from this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors: P(mu) = N(0.000000, 1000.000000), P(beta) = Ga(1000.000000, 0.001000)\n"
     ]
    }
   ],
   "source": [
    "m0 = 0\n",
    "v0 = 1000\n",
    "beta_mean0 = 1\n",
    "beta_var0 = 1000\n",
    "b0 = beta_var0 / beta_mean0\n",
    "c0 = beta_mean0**2 / beta_var0\n",
    "print(\"Priors: P(mu) = N(%f, %f), P(beta) = Ga(%f, %f)\" % (m0, v0, b0, c0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need the update equations. These will take existing values of $m$, $v$, $b$ and $c$ and produce new estimates. By repeatedly iterating we will converge on the optimal posterior hyperparameters.\n",
    "\n",
    "Here we implement the update equations as a Python function which takes values of $m$, $v$, $b$ and $c$ and returns updated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Equation 3.15 - these depend only on the data\n",
    "S1 = np.sum(DATA)\n",
    "S2 = np.sum(np.square(DATA))\n",
    "\n",
    "def update(m, v, b, c):\n",
    "    # Equation 3.17\n",
    "    m = (m0 + v0 * b * c * S1) / (1 + N * v0 * b * c)\n",
    "\n",
    "    # Equation 3.18\n",
    "    v = v0 / (1 + N * v0 * b * c)\n",
    "\n",
    "    # Equation 3.20\n",
    "    X = S2 - 2*S1*m + N * (m**2 + v)\n",
    "\n",
    "    # Equation 3.21\n",
    "    b = 1 / (1 / b0 + X / 2)\n",
    "    \n",
    "    # Equation 3.22\n",
    "    c = N / 2 + c0\n",
    "    \n",
    "    return m, v, b, c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterative process needs some starting values which we define similarly to the priors. We could in fact start off with the prior values. If the iterative process is working the starting values should not matter, however in more complex problems it is important to start out with reasonable values of the parameters or the iteration can become stuck in a local maximum and not find the best solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial values: (m, v, b, c) = (0.000000, 10.000000, 10.000000, 0.100000)\n"
     ]
    }
   ],
   "source": [
    "m = 0\n",
    "v = 10\n",
    "beta_mean1 = 1.0\n",
    "beta_var1 = 10\n",
    "b = beta_var1 / beta_mean1\n",
    "c = beta_mean1**2 / beta_var1\n",
    "print(\"Initial values: (m, v, b, c) = (%f, %f, %f, %f)\" % (m, v, b, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our initial values are not particularly close to the true values, so we are not cheating!\n",
    "\n",
    "Finally, let's iterate 10 times and see what happens to the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: (m, v, b, c) = (41.983297, 0.010000, 0.009544, 50.001000)\n",
      "Iteration 2: (m, v, b, c) = (41.982837, 0.020955, 0.009494, 50.001000)\n",
      "Iteration 3: (m, v, b, c) = (41.982833, 0.021064, 0.009494, 50.001000)\n",
      "Iteration 4: (m, v, b, c) = (41.982833, 0.021065, 0.009494, 50.001000)\n",
      "Iteration 5: (m, v, b, c) = (41.982833, 0.021065, 0.009494, 50.001000)\n",
      "Iteration 6: (m, v, b, c) = (41.982833, 0.021065, 0.009494, 50.001000)\n",
      "Iteration 7: (m, v, b, c) = (41.982833, 0.021065, 0.009494, 50.001000)\n",
      "Iteration 8: (m, v, b, c) = (41.982833, 0.021065, 0.009494, 50.001000)\n",
      "Iteration 9: (m, v, b, c) = (41.982833, 0.021065, 0.009494, 50.001000)\n",
      "Iteration 10: (m, v, b, c) = (41.982833, 0.021065, 0.009494, 50.001000)\n",
      "Inferred mean/precision of Gaussian: 41.982833, 0.474706\n",
      "Inferred variance on Gaussian mean/precision: 0.021065, 0.004507\n"
     ]
    }
   ],
   "source": [
    "for vb_iter in range(10):\n",
    "    m, v, b, c = update(m, v, b, c)\n",
    "    print(\"Iteration %i: (m, v, b, c) = (%f, %f, %f, %f)\" % (vb_iter+1, m, v, b, c))\n",
    "\n",
    "print(\"Inferred mean/precision of Gaussian: %f, %f\" % (m, c * b))\n",
    "print(\"Inferred variance on Gaussian mean/precision: %f, %f\" % (v, c * b**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update equations in this case converge within just a couple of iterations and lead to inferred hyperparameters close to our ground truth. Note also that we have inferred the variance on these parameters - this gives an indication of how confident we can be in their values. If you try reducing the number of samples in the data set the variance will increase since we have less information to infer $\\mu$ and $\\beta$.\n",
    "\n",
    "We can plot the inferred value of $\\mu$ with error bars derived from the inferred variance:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14990c83b370>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(DATA, \"rx\")\n",
    "plt.plot([MU_TRUTH] * N, \"g\")\n",
    "plt.plot([m] * N, \"b\")\n",
    "plt.plot([m+2*np.sqrt(v)] * N, \"b--\")\n",
    "plt.plot([m-2*np.sqrt(v)] * N, \"b--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Other things to try would include:\n",
    "\n",
    " - Reducing the variance of the priors, i.e. make them informative. This will cause the inferred values to move closer to the\n",
    "   prior values because we are now claiming we have prior knowledge of what $\\mu$ and $\\beta$ must be, and this can to some\n",
    "   extent override the information in the data.\n",
    "   \n",
    " - Changing the initial values of $m$, $v$, $b$ and $c$ to verify that the iteration can still converge to the correct    \n",
    "   solution.\n",
    " \n",
    " - Try modifying the ground truth values and verify that we still infer the correct solution.\n",
    " \n",
    " - Increasing the level of noise should cause the variance estimates in the parameters to go up\n",
    " - Reducing the number of data samples should also increase the variance estimates to go up since we have less information to go on.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple implementation of analytic Variational Bayes to infer a nonlinear forward model (optional)\n",
    "\n",
    "=============\n",
    "\n",
    "This implements section 4 of the FMRIB Variational Bayes tutorial 1\n",
    "for a single exponential decay model\n",
    "\n",
    "$g(\\theta)=A\\exp^{-\\lambda t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This starts the random number generator off with the same seed value each time, so the results are repeatable. However it is worth changing the seed (or simply removing this line) to see how different data samples affect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth parameters\n",
    "A_TRUTH = 42\n",
    "LAM_TRUTH = 1.5\n",
    "NOISE_PREC_TRUTH = 100\n",
    "NOISE_VAR_TRUTH = 1/NOISE_PREC_TRUTH\n",
    "NOISE_STD_TRUTH = np.sqrt(NOISE_VAR_TRUTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed data samples are generated by Numpy from the ground truth Gaussian distribution. Reducing the number of samples should make the inference less 'confident' - i.e. the output variances for MU and BETA will increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples are:\n",
      "[ 0.   0.2  0.4  0.6  0.8  1.   1.2  1.4  1.6  1.8  2.   2.2  2.4  2.6\n",
      "  2.8  3.   3.2  3.4  3.6  3.8  4.   4.2  4.4  4.6  4.8  5.   5.2  5.4\n",
      "  5.6  5.8  6.   6.2  6.4  6.6  6.8  7.   7.2  7.4  7.6  7.8  8.   8.2\n",
      "  8.4  8.6  8.8  9.   9.2  9.4  9.6  9.8 10.  10.2 10.4 10.6 10.8 11.\n",
      " 11.2 11.4 11.6 11.8 12.  12.2 12.4 12.6 12.8 13.  13.2 13.4 13.6 13.8\n",
      " 14.  14.2 14.4 14.6 14.8 15.  15.2 15.4 15.6 15.8 16.  16.2 16.4 16.6\n",
      " 16.8 17.  17.2 17.4 17.6 17.8 18.  18.2 18.4 18.6 18.8 19.  19.2 19.4\n",
      " 19.6 19.8]\n",
      "[4.20000000e+01 3.11143653e+01 2.30500887e+01 1.70759257e+01\n",
      " 1.26501569e+01 9.37146673e+00 6.94255331e+00 5.14316999e+00\n",
      " 3.81015404e+00 2.82263154e+00 2.09105687e+00 1.54909303e+00\n",
      " 1.14759634e+00 8.50160281e-01 6.29814226e-01 4.66577855e-01\n",
      " 3.45649376e-01 2.56063356e-01 1.89696400e-01 1.40530549e-01\n",
      " 1.04107591e-01 7.71248006e-02 5.71354576e-02 4.23269880e-02\n",
      " 3.13566040e-02 2.32295435e-02 1.72088691e-02 1.27486438e-02\n",
      " 9.44442762e-03 6.99660406e-03 5.18321177e-03 3.83981772e-03\n",
      " 2.84460693e-03 2.10733665e-03 1.56115338e-03 1.15653087e-03\n",
      " 8.56779143e-04 6.34717600e-04 4.70210363e-04 3.48340405e-04\n",
      " 2.58056919e-04 1.91173267e-04 1.41624640e-04 1.04918114e-04\n",
      " 7.77252503e-05 5.75802816e-05 4.26565218e-05 3.16007286e-05\n",
      " 2.34103955e-05 1.73428475e-05 1.28478975e-05 9.51795654e-06\n",
      " 7.05107563e-06 5.22356530e-06 3.86971235e-06 2.86675342e-06\n",
      " 2.12374317e-06 1.57330763e-06 1.16553496e-06 8.63449536e-07\n",
      " 6.39659149e-07 4.73871153e-07 3.51052384e-07 2.60066003e-07\n",
      " 1.92661633e-07 1.42727248e-07 1.05734946e-07 7.83303747e-08\n",
      " 5.80285688e-08 4.29886211e-08 3.18467538e-08 2.35926555e-08\n",
      " 1.74778691e-08 1.29479239e-08 9.59205791e-09 7.10597127e-09\n",
      " 5.26423300e-09 3.89983972e-09 2.88907232e-09 2.14027742e-09\n",
      " 1.58555651e-09 1.17460915e-09 8.70171862e-10 6.44639170e-10\n",
      " 4.77560443e-10 3.53785478e-10 2.62090728e-10 1.94161587e-10\n",
      " 1.43838441e-10 1.06558138e-10 7.89402103e-11 5.84803461e-11\n",
      " 4.33233060e-11 3.20946944e-11 2.37763344e-11 1.76139418e-11\n",
      " 1.30487290e-11 9.66673620e-12 7.16129431e-12 5.30521731e-12]\n",
      "[ 4.21764052e+01  3.11543810e+01  2.31479625e+01  1.73000150e+01\n",
      "  1.28369127e+01  9.27373894e+00  7.03756215e+00  5.12803427e+00\n",
      "  3.79983215e+00  2.86369139e+00  2.10546123e+00  1.69452038e+00\n",
      "  1.22370012e+00  8.62327782e-01  6.74200550e-01  4.99945287e-01\n",
      "  4.95057283e-01  2.35547529e-01  2.21003170e-01  5.51209753e-02\n",
      " -1.51191390e-01  1.42486660e-01  1.43579077e-01 -3.18895140e-02\n",
      "  2.58332066e-01 -1.22207024e-01  2.17847208e-02 -5.96974120e-03\n",
      "  1.62722349e-01  1.53932481e-01  2.06779543e-02  4.16560697e-02\n",
      " -8.59339678e-02 -1.95972310e-01 -3.32300615e-02  1.67914278e-02\n",
      "  1.23885847e-01  1.20872702e-01 -3.82624714e-02 -2.98819347e-02\n",
      " -1.04597240e-01 -1.41810620e-01 -1.70485394e-01  1.95182458e-01\n",
      " -5.08874929e-02 -4.37498499e-02 -1.25236879e-01  7.77806363e-02\n",
      " -1.61366374e-01 -2.12566852e-02 -8.95338082e-02  3.86997677e-02\n",
      " -5.10734627e-02 -1.18057995e-01 -2.81435312e-03  4.28360538e-02\n",
      "  6.65384598e-03  3.02487631e-02 -6.34310438e-02 -3.62732531e-02\n",
      " -6.72454051e-02 -3.59548423e-02 -8.13142772e-02 -1.72628000e-01\n",
      "  1.77428069e-02 -4.01779509e-02 -1.63019729e-01  4.62783039e-02\n",
      " -9.07297784e-02  5.19458257e-03  7.29090881e-02  1.28983147e-02\n",
      "  1.13940086e-01 -1.23482569e-01  4.02341737e-02 -6.84810020e-02\n",
      " -8.70797097e-02 -5.78849626e-02 -3.11552503e-02  5.61653636e-03\n",
      " -1.16514982e-01  9.00826499e-02  4.65662448e-02 -1.53624368e-01\n",
      "  1.48825220e-01  1.89588918e-01  1.17877957e-01 -1.79924834e-02\n",
      " -1.07075262e-01  1.05445173e-01 -4.03176946e-02  1.22244507e-01\n",
      "  2.08274979e-02  9.76639037e-02  3.56366397e-02  7.06573168e-02\n",
      "  1.05000209e-03  1.78587049e-01  1.26912093e-02  4.01989363e-02]\n"
     ]
    }
   ],
   "source": [
    "def model(t, a, lam):\n",
    "    \"\"\"\n",
    "    Simple exponential decay model\n",
    "    \"\"\"\n",
    "    return a * np.exp(-lam * t)\n",
    "\n",
    "\n",
    "N = 100\n",
    "DT = 0.2\n",
    "t = np.array([float(t)*DT for t in range(N)])\n",
    "DATA_CLEAN = model(t, A_TRUTH, LAM_TRUTH)\n",
    "DATA_NOISY = DATA_CLEAN + np.random.normal(0, NOISE_STD_TRUTH, [N])\n",
    "print(\"Data samples are:\")\n",
    "print(t)\n",
    "print(DATA_CLEAN)\n",
    "print(DATA_NOISY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Priors - noninformative because of high variance\n",
    "\n",
    "Note that the noise posterior is a gamma distribution with shape and scale parameters $s, c$. The mean here is $bc$ and the variance is $cb^2$. To make this more intuitive we define a prior mean and variance for the noise parameter BETA and express the prior scale and shape parameters in terms of these.\n",
    "\n",
    "So long as the priors stay noninformative they should not have a big impact on the inferred values - this is the point of noninformative priors. However if you start to reduce the prior variances the inferred values will be drawn towards the prior values and away from the values suggested by the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = 1.0\n",
    "a_var0 = 1000\n",
    "lam0 = 1.0\n",
    "lam_var0 = 1.0\n",
    "\n",
    "beta_mean0 = 1\n",
    "beta_var0 = 1000\n",
    "# c=scale, s=shape parameters for Gamma distribution\n",
    "c0 = beta_var0 / beta_mean0\n",
    "s0 = beta_mean0**2 / beta_var0\n",
    "\n",
    "# Priors as vectors/matrices - M=means, C=covariance, P=precision\n",
    "M0 = np.array([a0, lam0])\n",
    "C0 = np.array([[a_var0, 0], [0, lam_var0]])\n",
    "P0 = np.linalg.inv(C0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_jacobian(M):\n",
    "    \"\"\"\n",
    "    Numerical differentiation to calculate Jacobian matrix\n",
    "    of partial derivatives of model prediction with respect to\n",
    "    parameters\n",
    "    \"\"\"\n",
    "    J = None\n",
    "    for param_idx, param_value in enumerate(M):\n",
    "        ML = np.array(M)\n",
    "        MU = np.array(M)\n",
    "        delta = param_value * 1e-5\n",
    "        if delta < 0:\n",
    "            delta = -delta\n",
    "        if delta < 1e-10:\n",
    "            delta = 1e-10\n",
    "            \n",
    "        MU[param_idx] += delta\n",
    "        ML[param_idx] -= delta\n",
    "        \n",
    "        YU = model(t, MU[0], MU[1])\n",
    "        YL = model(t, ML[0], ML[1])\n",
    "        if J is None:\n",
    "            J = np.zeros([len(YU), len(M)], dtype=np.float32)\n",
    "        J[:, param_idx] = (YU - YL) / (2*delta)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_params(k, M, P, s, c, J):\n",
    "    \"\"\"\n",
    "    Update model parameters\n",
    "\n",
    "    From section 4.2 of the FMRIB Variational Bayes Tutorial I\n",
    "    \n",
    "    k = data - prediction\n",
    "    M = means (prior = M0)\n",
    "    P = precision (prior=P0)\n",
    "    s = noise shape (prior = s0)\n",
    "    c = noise scale (prior = c0)\n",
    "    J = Jacobian\n",
    "    \"\"\"\n",
    "    P_new = s*c*np.dot(J.transpose(), J) + P0\n",
    "    C_new = np.linalg.inv(P_new)\n",
    "    M_new = np.dot(C_new, (s * c * np.dot(J.transpose(), (k + np.dot(J, M))) + np.dot(P0, M0)))\n",
    "    return M_new, P_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_noise(k, P, J):\n",
    "    \"\"\"\n",
    "    Update noise parameters\n",
    "\n",
    "    From section 4.2 of the FMRIB Variational Bayes Tutorial I\n",
    "    \n",
    "    k = data - prediction\n",
    "    P = precision (prior=P0)\n",
    "    J = Jacobian\n",
    "    \"\"\"\n",
    "    C = np.linalg.inv(P)\n",
    "    c_new = N/2 + c0\n",
    "    s_new = 1/(1/s0 + 1/2 * np.dot(k.transpose(), k) + 1/2 * np.trace(np.dot(C, np.dot(J.transpose(), J))))\n",
    "    return c_new, s_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: A=1.000000, lam=1.000000, noise=0.000000\n"
     ]
    }
   ],
   "source": [
    "# Initial posterior parameters\n",
    "M = np.array([1.0, 1.0])\n",
    "C = np.array([[1.0, 0], [0.0, 1.0]])\n",
    "P = np.linalg.inv(C)\n",
    "c = 1e-8\n",
    "s = 50.0\n",
    "print(\"Iteration 0: A=%f, lam=%f, noise=%f\" % (M[0], M[1], c*s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: A=1.052030, lam=0.999984, noise=0.239626\n",
      "Iteration 2: A=36.244858, lam=3.185952, noise=0.366634\n",
      "Iteration 3: A=40.805754, lam=-0.031724, noise=0.763868\n",
      "Iteration 4: A=4.471958, lam=-0.024257, noise=0.006445\n",
      "Iteration 5: A=4.893654, lam=0.051933, noise=0.257984\n",
      "Iteration 6: A=9.288650, lam=0.222892, noise=0.387081\n",
      "Iteration 7: A=21.182487, lam=0.666577, noise=0.491758\n",
      "Iteration 8: A=37.409614, lam=1.339456, noise=0.755284\n",
      "Iteration 9: A=42.010610, lam=1.495240, noise=1.031156\n",
      "Iteration 10: A=42.096682, lam=1.493100, noise=1.048477\n",
      "Iteration 11: A=42.097375, lam=1.493144, noise=1.048511\n",
      "Iteration 12: A=42.097370, lam=1.493144, noise=1.048511\n",
      "Iteration 13: A=42.097381, lam=1.493145, noise=1.048511\n",
      "Iteration 14: A=42.097368, lam=1.493144, noise=1.048511\n",
      "Iteration 15: A=42.097381, lam=1.493145, noise=1.048511\n",
      "Iteration 16: A=42.097368, lam=1.493144, noise=1.048511\n",
      "Iteration 17: A=42.097381, lam=1.493145, noise=1.048511\n",
      "Iteration 18: A=42.097368, lam=1.493144, noise=1.048511\n",
      "Iteration 19: A=42.097381, lam=1.493145, noise=1.048511\n",
      "Iteration 20: A=42.097368, lam=1.493144, noise=1.048511\n"
     ]
    }
   ],
   "source": [
    "for idx in range(20):\n",
    "    k = DATA_NOISY - model(t, M[0], M[1])\n",
    "    J = calc_jacobian(M)\n",
    "    M, P = update_model_params(k, M, P, s, c, J)\n",
    "    c, s = update_noise(k, P, J)\n",
    "    print(\"Iteration %i: A=%f, lam=%f, noise=%f\" % (idx+1, M[0], M[1], c*s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyDeepLearning-1.1",
   "language": "python",
   "name": "pydeeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
